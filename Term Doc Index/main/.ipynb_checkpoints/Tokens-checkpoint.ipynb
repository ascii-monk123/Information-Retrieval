{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "short-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-authentication",
   "metadata": {},
   "source": [
    "<h2>Generating Word Tokens</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "casual-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "classical-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz.txt\n",
      "doomEternal.txt\n",
      "twice.txt\n",
      "maroon5.txt\n",
      "coldplay.txt\n",
      "bioshock.txt\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "directory=\"/home/aahan/Documents/Academic/Information Retrieval/corpus/\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        with open(directory+filename) as fh:\n",
    "            text=fh.read()\n",
    "            toks=list(map(lambda s:s.lower(),word_tokenize(text)))\n",
    "            tokens=tokens+toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "traditional-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['used', 'rock', 'ranked', 'nintendo', 'also', 'sound', '3', 'political', 'main', 'met', 'y', 'partnership', 'woman', '222', 'rolling', 'republic', 'eternal', '2020', 'signed', 'exterminate', 'reviews', 'serious', 'canadian', 'involved', 'id', 'from', 'over', 'likey', '2000s', 'attributes', 'be', '3.75', 'long', 'several', '``', \"''\", 'soundtrack', 'adult', 'alien', '2007', 'ball', 'help', '2008', 'concept', 'own', 'action', 'well', 'announced', 'rush', 'aboard', 'gods', 'gt', 'themes', 'campaigns', 'who', 'manga', 'chaeyoung', 'demon', '1', 'worldwide', 'six', 'million', '2019', 'mercury', 'system', 'between', 'digital', 'representing', 'foil', '1996', 'currently', 'mnet', 'vocalist', 'themselves', 'in', 'multiplayer', 'bassist', 'controls', 'mickey', 'chapters', '29', 'pronounced', 'cancer', 'mode', 'rescues', 'guitarist', 'brian', 'death', 'elizabeth', 'ドラゴンボールz', 'consumption', 'twicetagram', 'live', 'more', 'open', 'pursuing', 'international', 'campaign', 'port', '1986', 'piccolo', 'combat', 'remastered', 'two', 'she', 'dynamic', 'seven', 'had', 'manipulate', 'label', 'love', 'alongside', 'villains', 'provides', 'cheer', 'warring', 'fourth-best', 'successful', 'about', 'collaborator', '2015', 'signal', 'defend', 'underclass', 'starfish', 'exceptionalism', 'rankings', 'performed', 'recent', 'their', 'europe', 'aliens', 'strange', 'like', 'manchester', 'highest', 'his', 'conflict', 'end', 'space-time', 'imitated', 'oricon', 'aided', 'first-person', 'charts', 'industry', 'fancy', 'been', 'versions', 'once', 'aid', 'visual', 'flynn', 'charity', '2006', 'madden', 'media', 'its', 'point', 'number', 'was', 'global', 'though', 'defeats', '15', '4', 'toriyama', 'storytelling', 'december', 'sold', 're-emerged', 'five', 'wonder', 'sea', 'carmichael', 'direction', 'former', 'sport', 'make', 'jesse', 'loved', 'up', 'number-one', 'cd', 'bioshock', 'mina', 'underground', 'japan', 'a', 'awards', 'movement', 'version', 'infinite', 'high', 'soon', 'during', 'debut', 'nation', '1995', 'generation', 'blue', 'earned', '1996–1997', 'hell', 'previous', 're-released', 'room', 'able', 'playstation', 'maykr', 'ai-controlled', 'eno-produced', 'gameplay', 'year-end', 'member', 'ioshock', 'original', 'reprise', 'ken', 'turn', 'uses', 'tears', 'gaon', ':', 'nine', 'benefit', 'slayer', 'achieved', 'than', 'accolades', 'display', 'become', 'on', 'coaster-like', 'irrational', 'may', 'before', 'buckland', 'sections', 'hepburn', 'historical', '2012', 'offer', 'battlemode', 'playing', 'overall', 'and', 'cell', 'spawned', 'occupy', '1997', 'series', 'lead', 'os', 'play', 'mission', 'possesses', 'will', 'against', 'show', 'parlophone', 'twice', '135', 'place', '2002', 'which', 'pianist', 'xbox', 'many', 'female', 'set', 'voted', '—', 'prize', '34', 'held', 'consecutive', 'clothing', 'son', 'since', 'part', 'billboard', '2k', 'association', 'singles', 'underwater', 'shooter', 'television', '1.2', 'award', 'secret', 'september', 'weapons', 'along', 'members', 'third-highest', 'for', 'were', 'college', 'ghost', 'each', 'directed', '28', 'focus', 'year', 'joint', 'features', 'pectoralz', '5', 'ep', '2021', 'octone', 'best-selling', 'social', 'storyline', 'released', 'century', 'dragon', 'disliked', 'ninth', 'june', 'martin', 'oxfam', 'relief', 'influences', 'spaces', 'platinum', 'commonly', 'tzuyu', 'magical', 'an', ',', 'of', 'states', 'interscope', 'most', '1988', 'career', 'supported', 'praise', 'city', 'became', 'review', 'riaj', 'new', 'both', 'overexposed', 'choreography—including', 'safety', 'regarded', 'horde', 'nayeon', 'jane', '[', 'left', 'k-pop', 'combination', 'january', '9', '2013', 'full', 'represent', 'maxi', 'animation', 'central', 'majority', 'embark', 'haiti', 'viz', 'tt', 'another', 'teenage', 'changing', 'that', 'sixth', 'microsoft', 'gaming', 'festival', 'keep', 'development', 'shōnen', 'abbreviated', 'airborne', 'aggregator', 'upon', 'making', '13', 'stadia', 'stories', 'nominations', 'projects', 'jagger', \"n't\", 'twicecoaster', 'causes', 'dropped', 'not', 'called', '2015–2018', 'long-time', 'player', '2001', 'adventures', 'players', 'booker', 'story-based', 'three', 'wrist', 'ran', '12', 'nativist', 'according', 'due', 'standalone', 'crazes', 'pj', 'korean', 'adding', 'makes', 'success', 'contrast', 'feel', 'hands', 'recorded', 'fight', 'majin', 'adaptations', 'asian', 'levine', 'he', '360', 'digging', 'pre-release', 'fifth', 'art', 'publications', 'rhythm', 'sugar', 'links', 'came', 'studio', 'adapts', 'matt', 'while', 'gohan', 'icon', 'similar', 'through', 'then', 'separate', 'vigors', 'two-part', 'dahyun', 'populi', 'her', 'anime', 'dusick', '1893', 'this', 'grammy', 'category', 'published', 'girls', 'musician', 'j', 'metacritic', 'single', 'concepts', 'doom', 'softworks', 'with', 'events', 'months', 'blues', 'consoles', 'best', 'citizen', 'strive', '19', 'mechanics', 'factions', 'topped', 'time', 'rail', 'protagonist', 'compilation', 'earthquake', 'jump', 'design', 'fame', 'trade', 'tour', 'androids', 'vegeta', 'promotions', 'content', 'exposition', 'received', 'united', 'four', '200', 'produced', '2018', 'la', 'unique', 'predecessors', 'include', '.', 'relationship', 'gear', 'london', '1998', 'critics', 'increased', 'copies', 'switch', '10', 'bethesda', 'one', 'response', 'british', 'eight', 'coldplay', 'demons', 'discovers', 'developed', 'humanity', 'titled', 'powers', 'z', ')', 'acclaim', 'eps', 'pop', 'songs', 'brit', 'franchise', 'doomguy', 'simultaneously', 'magazine', '2014', '1989', '1994', 'title', 'the', 'records', 'rated', 'already', 'trust', 'group', '2010', 'v', 'celebrities', 'included', 'continued', 'rule', 'albums', 'april', 'shoulder', 'music', 'particularly', 'sequel', 'friends', 'parallels', 'within', 'introduced', 'selling', '#', '1999', 'includes', 'linux', 'spheres', 'toei', 'garnered', 'software', 'based', 'young', 'broadcast', 'momo', '2017', 'harder', 'ryan', 'first', '(', 'dewitt', 'amnesty', 'game', 'units', 'find', 'single-player', 'berryman', 'ravage', 'limited', 'sky-line', 'there', 'respectively', 'them', 'won', '325', 'band', 'by', 'again', 'finally', 'jeongyeon', 'american', 'special', 'sam', 'still', 'jonny', 'composed', '3.7', '?', 'we', 'jihyo', '81', 'begins', 'setting', 'pinkerton', 'jordi', 'third', '2011', '트와이스', '2', 'breathe', 'began', 'e3', 'angeles', 'often', '2009', 'sana', 'champion', 'creative', 'went', 'followed', 'wide', 'consists', 'major', 'viral', '6', 'at', 'different', 'manager', 'focused', 'remains', 'under', 'platforming', 'certification', 'among', '2016', 'together', 's', 'release', 'level', 'earn', 'farrar', '85', 'harvey', 'addition', 'ancient', 'university', 'incorporating', 'rapture', 'japanese', '2005', 'now', 'you', 'replaced', 'entertainment', 'continues', 'weeks', 'world', 'captive', '11', 'flowers', 'retail', 'privileges', 'jyp', 'akira', 'dance', 'life', 'dlc', '14', 'platforms', 'best-performing', '7', 'history', 'fair', 'head', 'collection', 'rose', 'us', 'as', 'vida', 'such', 'goku', 'readers', 'november', 'zetto', 'me', 'immediately', 'act', 'artists', 'extended', 'everyday', 'hot', 'phil', 'vice', '&', 'granted', 'rivals', ';', 'latter', 'sales', 'los', 'doragon', 'topping', 'other', 'recording', 'signing', 'red', '35', 'it', 'americans', 'all', 'debuted', 'follows', '—became', '100', 'lineup', 'blood', 'video', \"'s\", 'tv', 'album', 'night', 'artist', 'initially', 'formed', 'earth', 'have', 'guy', 'created', 'least', '8', 'adam', 'nominated', 'q', 'record', 'white', 'into', 'some', 'fourth', 'x', 'mixed', 'dreams', 'considered', 'dubbed', 'venture', 'join', 'they', 'tepid', 'calling', 'california', 'being', 'later', '20', 'versa', 'takes', 'numerous', 'トゥワイス', 'forces', ']', 'chart', 'peaked', 'keyboardist', 'throughout', 'story', 'including', 'first-week', 'top', 'frieza', 'next', 'super', 'name', 'stone', 'kai', 'sixteen', 'graphics', 'yellow', 'pill', 'uk', 'morton', 'maroon', 're-releases', 'dbz', 'korea', 'windows', 'buu', 'plans', 'what', 'hostile', 'either', 'until', 'song', 'valentine', 'are', 'warner', 'xyloto', 'columbia', 'elite', 'learns', 'or', 'drummer', 'dome', 'same', 'following', 'memes', 'companions', 'countries', 'injuries', 'continuum', 'creatures', 'burial', 'weekly', 'parachutes', 'performing', 'downloadable', 'comprise', 'independent', 'sent', 'moves', 'recruited', 'popularity', 'school', 'policeman', '20th', 'melon', '2000', 'director', 'columbian', 'working', 'quadruple', 'suffering', 'mylo', 'merchandise', 'installment', 'girl', 'rejoined', '1912', 'chris', 'cultural', 'various', 'eventually', 'south', 'james', 'come', 'secrets', 'openness', 'critical', 'is', 'games', 'kara', 'years', 'viva', 'dark', 'mtv', 'lane', 'second', 'list', 'bōru', 'seventh', 'domestic', 'fuji', 'has', 'after', 'numbers', 'can', 'eyes', 'reached', 'rebels', 'aired', 'roller', 'multi-instrumentalist', 'hope', 'vox', 'agent', 'self-releasing', 'psychokinetic', 'charted', 'founders', 're-releasing', 'october', 'to', 'march', 'stylized', 'wo', 'program']\n"
     ]
    }
   ],
   "source": [
    "tokens=list(set(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "serial-christopher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ranked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nintendo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens\n",
       "0      used\n",
       "1      rock\n",
       "2    ranked\n",
       "3  nintendo\n",
       "4      also"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(tokens,columns=[\"tokens\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-millennium",
   "metadata": {},
   "source": [
    "<h2> Creating Term Document Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "through-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz.txt\n",
      "doomEternal.txt\n",
      "twice.txt\n",
      "maroon5.txt\n",
      "coldplay.txt\n",
      "bioshock.txt\n"
     ]
    }
   ],
   "source": [
    "docIndices=dict()\n",
    "#intialize dictionary\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        fn=filename.split(\".\")[0]\n",
    "        docIndices[fn]=[]\n",
    "#create term document matrix\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        fn=filename.split(\".\")[0]\n",
    "\n",
    "        with open(directory+filename) as fh:\n",
    "            text=fh.read()\n",
    "            toks=list(map(lambda s:s.lower(),word_tokenize(text)))\n",
    "        \n",
    "            #assign token values\n",
    "            for tk in tokens:\n",
    "                if tk in toks:\n",
    "                    docIndices[fn].append(1)\n",
    "                else:\n",
    "                    docIndices[fn].append(0)\n",
    "                \n",
    "                    \n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "vietnamese-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz : 897\n",
      "doomEternal : 897\n",
      "twice : 897\n",
      "maroon5 : 897\n",
      "coldplay : 897\n",
      "bioshock : 897\n"
     ]
    }
   ],
   "source": [
    "for key in docIndices.keys():\n",
    "    print(\"{} : {}\".format(key,len(docIndices[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-seller",
   "metadata": {},
   "source": [
    "<h2>Create Dataframe for Documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "rocky-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(docIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "colonial-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0    0            0      0        0         0         1\n",
       "1    0            0      0        1         1         0\n",
       "2    0            0      1        0         0         0\n",
       "3    0            1      0        0         0         1\n",
       "4    1            0      0        0         1         1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-sample",
   "metadata": {},
   "source": [
    "<h2>Merge Into One Document Term Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "handmade-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rock</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ranked</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nintendo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>also</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens  dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0      used    0            0      0        0         0         1\n",
       "1      rock    0            0      0        1         1         0\n",
       "2    ranked    0            0      1        0         0         0\n",
       "3  nintendo    0            1      0        0         0         1\n",
       "4      also    1            0      0        0         1         1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index=pd.concat(objs=[df,df2],axis=1)\n",
    "term_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unable-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to disk\n",
    "term_index.to_csv(\"../data/term_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elect-constant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>used</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rock</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ranked</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>nintendo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>also</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    tokens  dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0           0      used    0            0      0        0         0         1\n",
       "1           1      rock    0            0      0        1         1         0\n",
       "2           2    ranked    0            0      1        0         0         0\n",
       "3           3  nintendo    0            1      0        0         0         1\n",
       "4           4      also    1            0      0        0         1         1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/term_matrix.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /home/aahan/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/aahan/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-authentication",
   "metadata": {},
   "source": [
    "<h2>Generating Word Tokens</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "casual-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "classical-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz.txt\n",
      "doomEternal.txt\n",
      "twice.txt\n",
      "maroon5.txt\n",
      "coldplay.txt\n",
      "bioshock.txt\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "directory=\"/home/aahan/Documents/Academic/Information Retrieval/corpus/\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        with open(directory+filename) as fh:\n",
    "            text=fh.read()\n",
    "            toks=list(map(lambda s:s.lower(),word_tokenize(text)))\n",
    "            tokens=tokens+toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "traditional-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['live', 'may', 'will', 'according', 'links', 'partnership', 'domestic', 'トゥワイス', 'previous', 'member', 'playstation', 'political', 'gohan', 'game', 'grammy', 'based', 'often', '7', 'university', 'manipulate', 'album', 'left', 'rolling', 'installment', 'dbz', 'yellow', 'while', 'campaign', 'long', '.', 'controls', 'along', 'policeman', 'earn', 'next', 'playing', 'serious', 'aggregator', 'nativist', 'seven', 'finally', 'this', 'stylized', 'continued', ']', 'set', 'met', 'continuum', 'introduced', 'os', 'mixed', 'most', 'popularity', 'songs', 'main', 'v', '2021', 'representing', 'since', 'formed', 'united', 'breathe', 'xbox', 'mina', 'red', '10', 'magical', 'special', '&', 'television', 'sections', 'fourth', 'digging', 'you', '2k', '2016', 'her', 'its', 'considered', 'aid', 'five', 'he', 'secret', 'granted', 'pj', 'college', 'help', 'came', 'topping', 'softworks', 'find', 'one', 'mtv', 'consoles', 'choreography—including', 'roller', 'world', 'both', '1912', 'causes', 'conflict', '14', 'vida', 'events', 're-releasing', 'level', 'vocalist', '8', '2010', 'act', 'son', 'majin', '2014', 'success', 'california', 'release', 'oricon', 'rated', 'creatures', 'valentine', 'record', 'number', 'benefit', 'review', 'show', 'able', 'action', 'though', 'sales', 'high', 'buckland', ';', 'extended', 'broadcast', '2', 'together', 'carmichael', 'place', 'magazine', '1998', 'viva', 'spawned', 'id', 'historical', 'warring', 'metacritic', '[', 'jane', 'time', 'white', 'weekly', 'to', 'coaster-like', 'won', 'adapts', 'interscope', 'association', 'doomguy', 'movement', 'become', '85', '3.7', 'fancy', 'commonly', 'androids', 'come', 'relief', 'eventually', 'regarded', 'voted', 'produced', 'tepid', 'rapture', 'first', 'jonny', 'international', 'received', '360', 'riaj', '15', 'airborne', 'point', 'included', 'features', 'influences', 'charity', 'twicecoaster', 'award', 'parachutes', 'changing', 'slayer', ':', 'farrar', 'represent', 'states', 'begins', 'design', 'keep', 'dance', 'an', 'collection', 'mission', 'foil', 'of', 'dragon', 'toriyama', 'burial', 'everyday', 'frieza', 'plans', 'third', 'cheer', 'coldplay', 'japan', 'numerous', 'gaming', 'strive', 'increased', 'harvey', 'concepts', 'pill', 'rhythm', 'jump', 'independent', 'shoulder', 'stories', 'ken', 'embark', 're-releases', 'elizabeth', 'particularly', 'earthquake', 'records', 'than', '1995', 'debut', 'guy', 'projects', 'concept', 'numbers', 'december', 'vox', 'million', '11', 'series', 'toei', 'aired', 'continues', 'two-part', 'chapters', 'tt', 'least', '2008', 'includes', 'predecessors', 'sequel', 'and', 'girl', 'stadia', 'seventh', 'wrist', 'more', 'limited', 'central', 'standalone', 'pronounced', 'chris', 'critical', 'signed', 'player', '222', 'ai-controlled', 'make', 'protagonist', 'achieved', 'between', '(', 'june', 'disliked', 'akira', '2000s', '19', 'composed', 'animation', '1989', 'stone', 'also', 'strange', 'offer', 'hepburn', 'soon', 'performed', '1994', 'through', \"''\", '``', 'media', 'girls', 'weeks', 'aliens', 'agent', '1996–1997', 'entertainment', 'games', 'mechanics', 'fame', 'rose', 'garnered', 'recruited', 'openness', 'years', 'aboard', 'vice', 'earth', 'consecutive', 'us', 'followed', 'debuted', 'possesses', 'woman', 'sky-line', 'nation', 'privileges', 'battlemode', 'imitated', '28', 'underground', 'angeles', 'nominated', 'space-time', 'graphics', 'blue', 'similar', 'defend', 'xyloto', 'number-one', 'manager', 'jesse', 'injuries', 'dark', 'storyline', 'best-performing', 'lead', 'wide', 'q', 'x', 'weapons', 'adaptations', 'ghost', 'occupy', 'critics', 'all', 'several', 'she', 'another', 'went', 'brit', 'linux', 'kai', 's', 'from', 'respectively', 'tears', 'clothing', 'retail', 'such', 'city', 'republic', 'horde', 'platforming', 'exterminate', 'praise', 'group', '2015', 'had', 'reached', 'fourth-best', 'accolades', 'haiti', 'direction', 'dewitt', 'discovers', 'ep', 'players', 'makes', 'open', 'are', 'parlophone', 'anime', 'gt', 'twice', 'sam', 'bassist', 'goku', 'contrast', 'visual', 'brian', 'likey', '2011', 'levine', 'canadian', 'kara', 'year', 'suffering', 'creative', '2018', 'not', '3.75', 'part', 'calling', 'starfish', 'adult', 'martin', 'albums', 'celebrities', 'piccolo', 'became', '2001', 'combat', 'school', 'trade', 'life', 'some', 'countries', 'first-week', 'music', 'icon', 'working', 'chaeyoung', '트와이스', 'gods', 'sea', 'making', 'title', 'ancient', 'them', 'harder', '100', 'different', 'dubbed', 'defeats', 'signal', 'mickey', 'berryman', 'oxfam', 'jihyo', 'that', 'focused', 'crazes', 'rush', 'be', 'rescues', 'same', 'either', 'exceptionalism', 'recent', 'jordi', 'video', 'ranked', '1', 'collaborator', 'peaked', 'lane', 'january', 'november', '20', '2017', 'mylo', 'october', 'hope', 'recording', 'up', 'best-selling', 'sound', 'publications', 'pre-release', 'populi', 'nominations', 'prize', 'name', 'follows', 'maroon', 'was', 'hot', 'factions', 'selling', 'have', 'generation', 'sport', 'secrets', 'pectoralz', 'switch', 'story', 'lineup', 'units', 'under', 'fight', 'nine', 'mnet', 'replaced', 'used', 'e3', 'directed', 'korean', 'columbian', 'k-pop', 're-emerged', 'tour', 'dynamic', 'focus', 'copies', 'ball', 'chart', 'pianist', 'awards', '2015–2018', 'bethesda', 'single-player', 'momo', 'into', 'guitarist', 'now', 'bioshock', 'adam', 'room', 'microsoft', 'digital', 'who', 'doom', 'rejoined', 'their', 'certification', 'drummer', 'pop', '2006', \"'s\", 'octone', 'it', 'charted', 'long-time', 'called', 'art', 'dlc', 'first-person', '—became', '29', 'published', 'champion', 'throughout', 'ravage', 'march', 'alien', '—', 'among', 'jyp', '1996', 'industry', 'teenage', 'we', 'hands', 'adding', 'list', 'sana', 'jagger', 'development', 'is', 'fair', 'due', 'developed', 'simultaneously', 'shōnen', '1997', 'remains', 'join', 'career', 'j', 'viral', 'various', 'category', 'his', 'british', 'turn', 'still', 'quadruple', 'or', 'melon', 'americans', 'artist', 'what', 'loved', 'friends', 'within', 'many', 'original', 'members', 'memes', 'well', 'include', 'incorporating', 'alongside', 'dome', 'combination', 'involved', 'overall', 'manchester', 'nayeon', 'mercury', 'response', 'dropped', 'highest', 'currently', 'rankings', 'multi-instrumentalist', 'joint', 'year-end', 'following', 'singles', '135', 'cancer', 'keyboardist', 'themes', 'gear', 'spheres', 'studio', 'los', 'matt', 'display', 'there', 'for', 'downloadable', 'elite', '?', 'september', 'adventures', 'vegeta', 'merchandise', 'remastered', 'social', 'provides', 'abbreviated', '#', 'blues', 'storytelling', 'soundtrack', 'sold', 'versa', 'ryan', 'night', 'ドラゴンボールz', 'phil', 'new', 'twicetagram', 'about', 'second', '13', 'recorded', '35', '9', 'citizen', 'underwater', 'before', 'system', '2000', 'other', '2013', 'wonder', 'shooter', 'eyes', 'ran', 'fifth', 'female', 'separate', 'south', 'me', 'own', 'major', 'during', 'sent', 'spaces', 'festival', 'pursuing', 'song', 'the', 'rule', 'billboard', 'fuji', 'history', 'performing', 'james', \"n't\", 'franchise', 'campaigns', 'by', 'story-based', 'mode', 'program', 'as', 'tv', 'dahyun', 'readers', 'london', 'madden', '2007', '3', '2005', 'reviews', 'later', 'after', 'z', 'buu', 'being', 'la', '2012', 'three', 'asian', 'relationship', 'eight', 'nintendo', '2019', 'april', 'released', 'has', 'booker', 'captive', 'held', 'eno-produced', 'feel', 'founders', 'rail', 'at', 'successful', 'safety', 'announced', 'columbia', 'earned', 'young', '200', 'blood', '34', 'which', 'irrational', 'like', 'latter', 'created', 'worldwide', 'multiplayer', 'topped', 'reprise', 'play', 'aided', '1.2', 'content', 'wo', 'moves', 'takes', 'rebels', 'with', 'immediately', 'full', 'exposition', 'promotions', '1988', '325', 'top', 'port', 'began', 'pinkerton', '12', 'in', 'demon', 'tzuyu', 'doragon', 'gaon', 'ioshock', 'end', 'six', 'ninth', 'zetto', 'supported', 'venture', 'versions', 'they', '20th', 'once', '1986', 'europe', 're-released', 'can', 'flynn', '2020', 'themselves', 'again', 'platinum', 'maykr', 'against', 'sugar', 'then', 'gameplay', 'over', 'majority', 'forces', 'platforms', 'jeongyeon', 'rivals', 'were', '2002', 'charts', 'third-highest', 'y', ',', 'former', 'american', 'best', 'already', '4', 'consumption', 'demons', 'four', 'psychokinetic', 'months', 'vigors', 'cultural', 'hell', 'amnesty', 'cd', 'eps', 'death', 'single', 'global', 'japanese', 'super', 'korea', 'maxi', 'attributes', 'software', 'love', 'addition', 'hostile', '1893', '81', 'director', 'head', 'viz', '1999', 'windows', 'century', 'uk', 'bōru', 'artists', 'self-releasing', 'musician', 'uses', 'trust', 'signing', 'cell', 'sixth', 'initially', 'rock', 'powers', 'band', 'version', 'humanity', 'parallels', 'sixteen', 'infinite', 'titled', 'until', 'consists', 'been', 'overexposed', 'dreams', '2009', 'setting', 'underclass', 'villains', 'companions', 'acclaim', 'label', 'flowers', 'compilation', 'upon', 'each', 'unique', '5', 'manga', 'including', 'warner', 'a', ')', 'comprise', 'learns', 'eternal', 'on', 'dusick', 'morton', '6', 'two']\n"
     ]
    }
   ],
   "source": [
    "tokens=list(set(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serial-christopher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>links</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens\n",
       "0       live\n",
       "1        may\n",
       "2       will\n",
       "3  according\n",
       "4      links"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(tokens,columns=[\"tokens\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-millennium",
   "metadata": {},
   "source": [
    "<h2> Creating Term Document Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "through-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz.txt\n",
      "doomEternal.txt\n",
      "twice.txt\n",
      "maroon5.txt\n",
      "coldplay.txt\n",
      "bioshock.txt\n"
     ]
    }
   ],
   "source": [
    "docIndices=dict()\n",
    "#intialize dictionary\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        fn=filename.split(\".\")[0]\n",
    "        docIndices[fn]=[]\n",
    "#create term document matrix\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        fn=filename.split(\".\")[0]\n",
    "\n",
    "        with open(directory+filename) as fh:\n",
    "            text=fh.read()\n",
    "            toks=list(map(lambda s:s.lower(),word_tokenize(text)))\n",
    "        \n",
    "            #assign token values\n",
    "            for tk in tokens:\n",
    "                if tk in toks:\n",
    "                    docIndices[fn].append(1)\n",
    "                else:\n",
    "                    docIndices[fn].append(0)\n",
    "                \n",
    "                    \n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vietnamese-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbz : 897\n",
      "doomEternal : 897\n",
      "twice : 897\n",
      "maroon5 : 897\n",
      "coldplay : 897\n",
      "bioshock : 897\n"
     ]
    }
   ],
   "source": [
    "for key in docIndices.keys():\n",
    "    print(\"{} : {}\".format(key,len(docIndices[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-seller",
   "metadata": {},
   "source": [
    "<h2>Create Dataframe for Documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rocky-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(docIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "colonial-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0    0            0      0        0         1         0\n",
       "1    0            0      0        1         0         1\n",
       "2    0            1      0        1         1         0\n",
       "3    0            0      0        0         0         1\n",
       "4    0            0      0        0         0         1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-sample",
   "metadata": {},
   "source": [
    "<h2>Merge Into One Document Term Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handmade-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>live</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>links</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens  dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0       live    0            0      0        0         1         0\n",
       "1        may    0            0      0        1         0         1\n",
       "2       will    0            1      0        1         1         0\n",
       "3  according    0            0      0        0         0         1\n",
       "4      links    0            0      0        0         0         1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index=pd.concat(objs=[df,df2],axis=1)\n",
    "term_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unable-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to disk\n",
    "term_index.to_csv(\"../data/term_matrix.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "elect-constant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>dbz</th>\n",
       "      <th>doomEternal</th>\n",
       "      <th>twice</th>\n",
       "      <th>maroon5</th>\n",
       "      <th>coldplay</th>\n",
       "      <th>bioshock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>live</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>links</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens  dbz  doomEternal  twice  maroon5  coldplay  bioshock\n",
       "0       live    0            0      0        0         1         0\n",
       "1        may    0            0      0        1         0         1\n",
       "2       will    0            1      0        1         1         0\n",
       "3  according    0            0      0        0         0         1\n",
       "4      links    0            0      0        0         0         1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/term_matrix.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
